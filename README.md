# Gen-AI-Handson-unit_1-
# Unit 1 – Generative AI Hands-On Documentation

## Key Concepts Understood
- Difference between Encoder-only and Encoder–Decoder Transformer architectures.
- How BERT and RoBERTa perform Masked Language Modeling for text understanding.
- How BART performs sequence-to-sequence generation tasks.
- Use of Hugging Face pipelines for text generation, fill-mask, question answering, and summarization.
- Importance of fine-tuning for task-specific performance.

## Solution Implemented

### Assignment (Model Benchmark)
Implemented three experiments comparing BERT, RoBERTa, and BART on:
1. Text Generation
2. Fill-Mask Prediction
3. Question Answering

Observed model behavior and explained results based on architecture differences.

### Project (TL;DR for News Articles)
Implemented a text summarization system using Hugging Face summarization pipeline with `sshleifer/distilbart-cnn-12-6` model.  
The system takes long text as input and produces a concise summary.

## Conclusion
The hands-on exercises demonstrated how different transformer architectures behave on various NLP tasks and how pretrained Hugging Face models can be used to quickly build Generative AI applications.
