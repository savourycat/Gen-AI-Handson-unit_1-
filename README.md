# Gen-AI-Handson-unit_1-
# Unit 1 – Generative AI Hands-On Documentation

Key Concepts Gained
•	Understood the architectural differences between encoder-only and encoder–decoder Transformer models.
•	Learned how BERT and RoBERTa use Masked Language Modeling (MLM) to understand contextual meaning in text.
•	Explored how BART handles sequence-to-sequence tasks, making it suitable for generation-based applications.
•	Practiced using Hugging Face pipelines for tasks such as text generation, fill-mask prediction, question answering, and summarization.
•	Recognized the role of fine-tuning in improving model performance for specific NLP tasks.
Solution Implementation
Assignment: Model Benchmarking
Three experiments were conducted to compare BERT, RoBERTa, and BART across the following tasks:
1.	Text Generation
2.	Fill-Mask Prediction
3.	Question Answering
The performance and outputs of each model were analyzed and the results were explained based on their underlying architectures and design objectives.
Project: TL;DR for News Articles
A text summarization system was developed using the Hugging Face summarization pipeline with the bart-large-cnn model.
The system accepts long news articles as input and generates short, meaningful summaries that capture the key points of the text.
Conclusion
Through these hands-on experiments, I gained practical insight into how different Transformer architectures behave across various NLP tasks. The work also showed how pretrained models from Hugging Face can be effectively used to build real-world Generative AI applications with minimal effort.
